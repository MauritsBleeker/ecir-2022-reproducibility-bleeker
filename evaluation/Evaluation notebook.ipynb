{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path += [\"../\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.evaluation import utils\n",
    "from vsepp.model import VSE\n",
    "from vsepp.evaluation import evalrank as vsepp_evalrank\n",
    "from VSRN.evaluation import evalrank as vsrn_eval_rank\n",
    "from vsepp.vocab import Vocabulary\n",
    "import vsepp.evaluation\n",
    "from shared.evaluation.recall import RecallMetric\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rank_mean_std(path, f_eval_rank, split='test', n_experiments=5, model_file='model_best.pth.tar'):\n",
    "\n",
    "    recall = RecallMetric()\n",
    "    for i in range(0, n_experiments):\n",
    "        r_i2t, r_t2i, rsum, ar_i2t, ar_t2i = f_eval_rank(os.path.join(path, str(i), model_file), split=split)\n",
    "        recall.add_recals(r_i2t=r_i2t, r_t2i=r_t2i, rsum=rsum, ar_i2t=ar_i2t, ar_t2i=ar_t2i, map5=r_i2t[5], map10=r_i2t[6])\n",
    "    \n",
    "    print('Done')\n",
    "    recall.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "The <i>eval_rank_mean_std</i> function prints all the metrics we evaluate in this work for one loss function/dataset/method combination.\n",
    "\n",
    "To evaluate VSRN, give the <i>vsrn_eval_rank</i> function as input. To evalute VSE++, the <i>vsepp_eval_rank</i> should be given as input. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../VSRN/GCN_lib/Rs_GCN.py:34: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(self.W[1].weight, 0)\n",
      "../VSRN/GCN_lib/Rs_GCN.py:35: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(self.W[1].bias, 0)\n",
      "/home/mbleeke1/anaconda3/envs/MMC_reproduce/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/mbleeke1/anaconda3/envs/MMC_reproduce/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/mbleeke1/anaconda3/envs/MMC_reproduce/lib/python3.7/site-packages/torch/optim/adam.py:48: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  super(Adam, self).__init__(params, defaults)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "52\n",
      "Computing results...\n",
      "Images: 5000, Captions: 25000\n",
      "rsum: 402.0\n",
      "Average i2t Recall: 71.8\n",
      "Image to text: 49.3 78.6 87.5 2.0 6.6\n",
      "Average t2i Recall: 62.2\n",
      "Text to image: 38.3 68.8 79.3 2.0 24.2\n",
      "Loading dataset\n",
      "52\n",
      "Computing results...\n",
      "Images: 5000, Captions: 25000\n",
      "rsum: 399.3\n",
      "Average i2t Recall: 71.3\n",
      "Image to text: 48.6 77.8 87.6 2.0 16.7\n",
      "Average t2i Recall: 61.8\n",
      "Text to image: 38.1 68.2 79.0 2.0 26.5\n",
      "Loading dataset\n",
      "52\n",
      "Computing results...\n",
      "Images: 5000, Captions: 25000\n",
      "rsum: 398.4\n",
      "Average i2t Recall: 71.5\n",
      "Image to text: 48.9 78.2 87.5 2.0 6.5\n",
      "Average t2i Recall: 61.3\n",
      "Text to image: 37.2 67.7 78.9 2.0 18.5\n",
      "Loading dataset\n",
      "52\n",
      "Computing results...\n",
      "Images: 5000, Captions: 25000\n",
      "rsum: 400.3\n",
      "Average i2t Recall: 71.7\n",
      "Image to text: 49.5 78.3 87.2 2.0 6.4\n",
      "Average t2i Recall: 61.8\n",
      "Text to image: 38.1 68.3 78.9 2.0 24.8\n",
      "Loading dataset\n",
      "52\n",
      "Computing results...\n",
      "Images: 5000, Captions: 25000\n",
      "rsum: 395.0\n",
      "Average i2t Recall: 70.7\n",
      "Image to text: 47.9 77.3 87.0 2.0 6.5\n",
      "Average t2i Recall: 60.9\n",
      "Text to image: 37.2 67.3 78.3 2.0 21.0\n",
      "Done\n",
      "Task: i2t, Metric: r@1, \n",
      " Mean: 48.85 , Std: 0.56\n",
      "$ 48.85 \\pm 0.56 $\n",
      "______________\n",
      "Task: i2t, Metric: r@5, \n",
      " Mean: 78.05 , Std: 0.45\n",
      "$ 78.05 \\pm 0.45 $\n",
      "______________\n",
      "Task: i2t, Metric: r@10, \n",
      " Mean: 87.36 , Std: 0.23\n",
      "$ 87.36 \\pm 0.23 $\n",
      "______________\n",
      "Task: i2t, Metric: ar, \n",
      " Mean: 71.42 , Std: 0.38\n",
      "$ 71.42 \\pm 0.38 $\n",
      "______________\n",
      "Task: t2i, Metric: r@1, \n",
      " Mean: 37.80 , Std: 0.49\n",
      "$ 37.80 \\pm 0.49 $\n",
      "______________\n",
      "Task: t2i, Metric: r@5, \n",
      " Mean: 68.05 , Std: 0.53\n",
      "$ 68.05 \\pm 0.53 $\n",
      "______________\n",
      "Task: t2i, Metric: r@10, \n",
      " Mean: 78.89 , Std: 0.32\n",
      "$ 78.89 \\pm 0.32 $\n",
      "______________\n",
      "Task: t2i, Metric: ar, \n",
      " Mean: 61.58 , Std: 0.43\n",
      "$ 61.58 \\pm 0.43 $\n",
      "______________\n",
      "Task: rsum, Metric: None, \n",
      " Mean: 399.00 , Std: 2.32\n",
      "$ 399.00 \\pm 2.32 $\n",
      "______________\n",
      "Task: map5, Metric: None, \n",
      " Mean: 0.57 , Std: 0.01\n",
      "$ 0.57 \\pm 0.01 $\n",
      "______________\n",
      "Task: map10, Metric: None, \n",
      " Mean: 0.53 , Std: 0.00\n",
      "$ 0.53 \\pm 0.00 $\n",
      "______________\n"
     ]
    }
   ],
   "source": [
    "path = '{path to repo}/out/vsrn/out/coco/paper_experiments/triplet_max/'\n",
    "eval_rank_mean_std(path, vsrn_eval_rank, split='testall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '{path to repo}/out/vsrn/out/f30k/paper_experiments/triplet_max/'\n",
    "eval_rank_mean_std(path, vsrn_eval_rank, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '{path to repo}/out/vsepp/out/f30k/paper_experiments/triplet_max/'\n",
    "eval_rank_mean_std(path, vsepp_evalrank, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '{path to repo}/out/vsepp/out/coco/paper_experiments/triplet_max/'\n",
    "eval_rank_mean_std(path, vsepp_evalrank, split='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
